# Interesting-Papers-and-Articles

## Some classic papers
- [LeNet](http://yann.lecun.com/exdb/lenet/)
- [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
- [ResNet](https://arxiv.org/abs/1512.03385)
- [VGG](https://arxiv.org/abs/1409.1556)
- GoogleNet/Inception [paper 1](https://arxiv.org/pdf/1409.4842.pdf) and [paper 2](https://arxiv.org/pdf/1512.00567.pdf)
- [MobileNet](https://arxiv.org/abs/1704.04861) and [MobileNetV3](https://arxiv.org/abs/1905.02244)
- [SqueezeNet](https://arxiv.org/abs/1602.07360)
- [LSTMs](https://www.bioinf.jku.at/publications/older/2604.pdf)
- [GRUs: Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
- [GANs](https://arxiv.org/abs/1406.2661)

## NLP
- [Word2Vec](https://arxiv.org/abs/1301.3781)
- [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)
- [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf)
- [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078)
- [Tokenizers](https://blog.floydhub.com/tokenization-nlp/)
- [(Attention): Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Attention is all you need](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [ELMo: Deep contextualized word representations](https://arxiv.org/abs/1802.05365)
- BERT [article](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) and [paper](https://arxiv.org/abs/1810.04805)
- [ULMFiT](https://arxiv.org/abs/1801.06146)
- [GPT-2](https://openai.com/blog/better-language-models/) and [GPT-3](https://arxiv.org/abs/2005.14165)
- [Turing-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)
- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)
- [List of important NLP papers](https://github.com/mihail911/nlp-library)
- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- [XLNet](https://arxiv.org/abs/1906.08237)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
- [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)
- [ERNIE 2.0](https://arxiv.org/abs/1907.12412)
- [RoBERTa](https://arxiv.org/abs/1907.11692)
- [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5)
- [TinyBERT](https://arxiv.org/abs/1909.10351)
- [Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/abs/1909.05840)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/abs/1907.10529)
- [What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)
- [Imitation Attacks and Defenses for Black-box Machine Translation Systems](https://www.ericswallace.com/imitation)
- [WT5?! Training Text-to-Text Models to Explain their Predictions](https://arxiv.org/abs/2004.14546)
- [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
- [ICLR 2020 Papers, curated by HuggingFace](https://docs.google.com/document/d/17pnPvWglcerkjFLOn-3Y1lB4MQMO-eVlPXjJUKl8XDw/mobilebasic)
- [The Cost of Training NLP Models: A Concise Overview](https://arxiv.org/abs/2004.08900)
- Reformer: The Efficient Transformer [blog](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html) and [paper](https://arxiv.org/abs/2001.04451)
- [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)
- [The State and Fate of Linguistic Diversity and Inclusion in the NLP World](https://arxiv.org/abs/2004.09095)
- [Adversarial NLI: A New Benchmark for Natural Language Understanding](https://arxiv.org/abs/1910.14599)
- [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://arxiv.org/abs/1901.11196)
- [Towards a Human-like Open-Domain Chatbot (Meena)](https://arxiv.org/abs/2001.09977)
- Recipes for building an open-domain chatbot (a state-of-the-art open source chatbot) [post](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/) and [paper](https://arxiv.org/abs/2004.13637)
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- [The Evolved Transformer](https://arxiv.org/abs/1901.11117)
- [Decoding methods for language generation with Transformers (article)](https://huggingface.co/blog/how-to-generate)
- [AWD-LSTM: Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)
- [Single Headed Attention RNN: Stop Thinking With Your Head](https://arxiv.org/abs/1911.11423)
- Generating Long Sequences with Sparse Transformers [paper](https://arxiv.org/abs/1904.10509) and [article](https://openai.com/blog/sparse-transformer/)
- [A Survey of Long-Term Context in Transformers (article)](https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/)
- [On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing](https://www.aclweb.org/anthology/N19-1253.pdf)
- [Google's Language Interpretability Tool (LIT)](https://github.com/pair-code/lit)
- [Beyond Accuracy: Behavioral Testing of NLP models with CheckList](https://arxiv.org/abs/2005.04118)
- [Generative Language Modeling for Automated Theorem Proving](https://arxiv.org/abs/2009.03393)


## Model training (deep learning)
- [A Recipe for Training Neural Networks (Andrej Karpathy)](http://karpathy.github.io/2019/04/25/recipe/)
- [Multi-Task Learning (Sebastian Ruder)](http://ruder.io/multi-task/)
- [Optimizing Gradient Descent (Sebastian Ruder)](http://ruder.io/optimizing-gradient-descent/)
- [More on Optimizing Gradient Descent (Sebastian Ruder)](http://ruder.io/deep-learning-optimization-2017/)
- [Cyclical Learning Rates (Leslie Smith)](https://arxiv.org/abs/1506.01186)
- [Adam optimizer](https://arxiv.org/abs/1412.6980)
- [RAdam optimizer](https://arxiv.org/abs/1908.03265v1)
- [AdamW: Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)
- [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747) and [blog](https://ruder.io/optimizing-gradient-descent/)
- [Nesterov Momentum[(http://cs229.stanford.edu/proj2015/054_report.pdf)
- [LARS Optimizer: Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)
- [LAMB Optimizer: Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)
- [Adafactor: Adaptive Learning Rates with Sublinear Memory Cost](https://arxiv.org/abs/1804.04235)
- [Adabound and AMSBound](https://arxiv.org/abs/1902.09843)
- [Norm matters: efficient and accurate normalization schemes in deep networks](https://arxiv.org/abs/1803.01814)
- [Three Mechanisms of Weight Decay Regularization](https://arxiv.org/abs/1810.12281)
- [Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/abs/1607.01981)
- [Hyperparameters (Leslie Smith)](https://arxiv.org/abs/1803.09820)
- [Super-Convergence (Leslie Smith)](https://arxiv.org/abs/1708.07120)
- [BatchNorm](https://arxiv.org/abs/1502.03167)
- [LayerNorm](https://arxiv.org/abs/1607.06450)
- [InstanceNorm](https://arxiv.org/abs/1607.08022)
- [GroupNorm](https://arxiv.org/abs/1803.08494)
- [Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
- Targeted dropout [Paper](https://arxiv.org/abs/1905.13678) and [website](https://for.ai/blog/targeted-dropout/)
- [DropConnect](http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf)
- [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382)
- [Quantization](https://arxiv.org/abs/1907.05686)
- [Mixed Precision Training](https://arxiv.org/abs/1710.03740)
- [High-Accuracy Low-Precision Training](https://arxiv.org/abs/1803.03383)
- [Gate decorators for model pruning](https://arxiv.org/abs/1909.08174)
- [ZeRO-2 & DeepSpeed: Shattering barriers of deep learning speed & scale](https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/) and [paper](https://arxiv.org/abs/1910.02054)
- [All you need is a good init (LSUV)](https://arxiv.org/abs/1511.06422)
- [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321)
- [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
- [Kaiming initialization, and PReLU](https://arxiv.org/abs/1502.01852)
- [GELUs](https://arxiv.org/abs/1606.08415)
- [SELUs](https://arxiv.org/abs/1706.02515)
- [Orthogonal initialization](https://arxiv.org/abs/1312.6120)
- [A survey on Image Data Augmentation for Deep Learning](https://link.springer.com/article/10.1186/s40537-019-0197-0)
- [On the Generalization Effects of Linear Transformations in Data Augmentation](https://arxiv.org/abs/2005.00695)
- [MixUp Data Augmentation](https://arxiv.org/abs/1710.09412)
- [Manifold Mixup: Better Representations by Interpolating Hidden States](https://arxiv.org/abs/1806.05236)
- [CutMix Data Augmentation](https://arxiv.org/abs/1905.04899)
- [Learning to Compose Domain-Specific Transformations for Data Augmentation](https://arxiv.org/abs/1709.01643)
- [AutoAugment: Learning Augmentation Policies from Data](https://arxiv.org/abs/1805.09501)
- [RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/abs/1909.13719)
- [Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules](https://arxiv.org/abs/1905.05393)
- [Pooling is neither necessary nor sufficient for appropriate deformation stability in CNNs](https://arxiv.org/abs/1804.04438)
- [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407)
- [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612)
- [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)
- [NVIDIA TensorRT for inference](https://developer.nvidia.com/tensorrt)

## Object detection / semantic segmentation
- [Faster R-CNN](https://arxiv.org/abs/1506.01497)
- [YOLO](https://arxiv.org/abs/1506.02640), [YOLOv2](https://arxiv.org/abs/1612.08242), [YOLOv3](https://arxiv.org/abs/1804.02767)
- [YOLOv4](https://arxiv.org/abs/2004.10934)
- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144)
- [RetinaNet](https://arxiv.org/abs/1708.02002)
- [U-Net](https://arxiv.org/abs/1505.04597)
- [Fully Convolutional Networks](https://arxiv.org/abs/1411.4038)
- [DeepLabv3](https://arxiv.org/abs/1706.05587)
- [Mask R-CNN](https://arxiv.org/abs/1703.06870)

## Special topics in CV
- [OpenPose](https://arxiv.org/abs/1812.08008)
- [A Simple yet Effective Baseline for 3D Human Pose Estimation](http://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html)
- [CycleGAN](https://arxiv.org/abs/1703.10593)
- [BigGAN: Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096)
- [SAGAN: Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318)
- [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957)
- [Pix2Pix](https://arxiv.org/abs/1611.07004)
- [Polygon-RNN++](http://www.cs.toronto.edu/polyrnn/) and [paper](https://arxiv.org/abs/1803.09693)
- [Super SloMo](https://arxiv.org/abs/1712.00080)
- [Soccer on Your Tabletop](https://arxiv.org/abs/1806.00890)
- [PixelRNN](https://arxiv.org/abs/1601.06759)
- [PixelCNN](https://arxiv.org/abs/1606.05328)
- [FaceNet](https://arxiv.org/abs/1503.03832)
- [Beyond triplet loss: a deep quadruplet network for person re-identification (quadruplet loss)](https://arxiv.org/abs/1704.01719)
- [LayoutNet](https://arxiv.org/abs/1803.08999)
- [DeepPerimeter](https://arxiv.org/abs/1904.11595)
- [StyleGAN](https://arxiv.org/abs/1812.04948)
- [Recognizing Material Properties from Images](https://arxiv.org/abs/1801.03127)
- [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)
- EfficientNet [paper](https://arxiv.org/abs/1905.11946) and [blog post](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)
- EfficientDet [paper](https://arxiv.org/abs/1911.09070) and [blog post](https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html)
- [Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet](https://arxiv.org/abs/1904.00760)
- [pySLAM](https://github.com/luigifreda/pyslam)
- [The Origins and Prevalence of Texture Bias in Convolutional Neural Networks](https://arxiv.org/abs/1911.09071)
- [Image GPT: Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/)
- [DETR: End-to-end Object Detection with Transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers/)
- NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections (NeRF-W) [paper](https://arxiv.org/abs/2008.02268) and [examples](https://nerf-w.github.io/)
- [VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training](https://arxiv.org/abs/2009.13682)
- [Vision transformers](https://arxiv.org/abs/2010.11929)
- [DALL-E](https://openai.com/blog/dall-e/) and [DALL-E 2](https://openai.com/dall-e-2/)
- [Imagen](https://imagen.research.google/)
- [Parti](https://parti.research.google/)

## "AutoML"-ish ideas
- Fast AI thoughts on AutoML [part 1](https://www.fast.ai/2018/07/12/auto-ml-1/), [part 2](https://www.fast.ai/2018/07/16/auto-ml2/), [part 3](https://www.fast.ai/2018/07/23/auto-ml-3/)
- NASNet [article](https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html) and [paper](https://arxiv.org/abs/1707.07012)
- [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578)
- [Regularized Evolution for Image Classifier Architecture Search (AmoebaNet)](https://arxiv.org/abs/1802.01548)
- [Large-Scale Evolution of Image Classifiers](https://arxiv.org/abs/1703.01041)
- [ENAS](https://arxiv.org/abs/1802.03268)
- [MnasNet: Platform-Aware Neural Architecture Search for Mobile](https://arxiv.org/abs/1807.11626)
- [DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055) also see [this](https://www.fast.ai/2018/07/16/auto-ml2/)
- [FBNet](https://arxiv.org/abs/1812.03443)
- [AutoML-Zero: Evolving Machine Learning Algorithms From Scratch](https://arxiv.org/abs/2003.03384)

## Semi-supervised, Self-supervised, Weakly-supervised, and Unsupervised learning
- [MixMatch](https://arxiv.org/abs/1905.02249)
- [FixMatch](https://arxiv.org/abs/2001.07685)
- [Snorkel: Rapid Training Data Creation with Weak Supervision](https://arxiv.org/abs/1711.10160)
- [Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods](https://arxiv.org/abs/2002.11955)
- [Unsupervised Data Augmentation](https://arxiv.org/abs/1904.12848)
- [Revisiting Self-Supervised Visual Representation Learning](https://arxiv.org/abs/1901.09005)
- [Exploring the Limits of Weakly Supervised Pretraining](https://arxiv.org/abs/1805.00932)
- [Deep InfoMax](https://arxiv.org/abs/1808.06670)
- [Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks](http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf)
- [Self-training with Noisy Student improves ImageNet classification (pseudo labeling)](https://arxiv.org/abs/1911.04252) and [video](https://www.youtube.com/watch?v=Y8YaU9mv_us&t=4s)
- [Meta Pseudo Labels](https://arxiv.org/abs/2003.10580)
- Billion-scale semi-supervised learning for image classification [blog](https://ai.facebook.com/blog/billion-scale-semi-supervised-learning/) and [paper](https://arxiv.org/abs/1905.00546)
- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)
- [SimCLRv2: Big Self-Supervised Models are Strong Semi-Supervised Learners](https://arxiv.org/abs/2006.10029)
- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)
- [MoCo v2: Improved Baselines with Momentum Contrastive Learning](https://arxiv.org/abs/2003.04297)
- [Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning](https://arxiv.org/abs/2006.07733)
- [RotNet: Unsupervised Representation Learning by Predicting Image Rotations](https://arxiv.org/abs/1803.07728)
- [BiGAN: Adversarial Feature Learning](https://arxiv.org/abs/1605.09782)
- [DeepCluster: Deep Clustering for Unsupervised Learning of Visual Features](https://arxiv.org/abs/1807.05520)
- [SCAN: Learning to Classify Images without Labels](https://arxiv.org/abs/2005.12320)
- [Self-labelling via simultaneous clustering and representation learning](https://arxiv.org/abs/1911.05371)
- [S4L: Self-Supervised Semi-Supervised Learning](https://arxiv.org/abs/1905.03670)
- [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)
- [Data-Efficient Image Recognition with Contrastive Predictive Coding](https://arxiv.org/abs/1905.09272)
- [ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data](https://arxiv.org/abs/2001.07966)
- Image GPT [blog](https://openai.com/blog/image-gpt/) and [paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)
- Unsupervised Learning of Visual Features by Contrasting Cluster Assignments [blog](https://ai.facebook.com/blog/high-performance-self-supervised-image-classification-with-contrastive-clustering/) and [paper](https://arxiv.org/abs/2006.09882)

## "TinyML" / efficiency etc.
- [Why the Future of Machine Learning is Tiny (Pete Warden)](https://petewarden.com/2018/06/11/why-the-future-of-machine-learning-is-tiny/)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- [PyTorch mixed-precision training](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/)
- [PyTorch quantization](https://pytorch.org/docs/stable/quantization.html)
- [TF Lite](https://www.tensorflow.org/lite) and [TF Lite for Microcontrollers](https://www.tensorflow.org/lite/microcontrollers)
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
- [Rigging the Lottery: Making All Tickets Winners](https://arxiv.org/abs/1911.11134)
- [Bonsai: Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things](http://proceedings.mlr.press/v70/kumar17a.html)
- [A Cascade Architecture for Keyword Spotting on Mobile Devices](https://arxiv.org/abs/1712.03603)
- [NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications](https://arxiv.org/abs/1804.03230)
- [TinyLSTMs: Efficient Neural Speech Enhancement for Hearing Aids](https://arxiv.org/pdf/2005.11138.pdf)
- [MCUNet: Tiny Deep Learning on IoT Devices](https://arxiv.org/pdf/2007.10319.pdf)
- [Characterizing and Mitigating Bias in Compact Models](http://whi2020.online/static/pdfs/paper_73.pdf)
- [What Do Compressed Deep Neural Networks Forget?](https://arxiv.org/abs/1911.05248)
- [The State of Sparsity in Deep Neural Networks](https://arxiv.org/abs/1902.09574)
- [Fast Algorithms for Convolutional Neural Networks (Winograd convolutions)](https://arxiv.org/abs/1509.09308)
- Block-Sparse GPU Kernels [blog](https://openai.com/blog/block-sparse-gpu-kernels/) and [paper](https://cdn.openai.com/blocksparse/blocksparsepaper.pdf)

## Fairness
- [Datasheets for Datasets](https://arxiv.org/abs/1803.09010)
- [Gender shades: Intersectional accuracy disparities in commercial gender classification](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)
- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520)
- [Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations](https://arxiv.org/abs/1811.08489)
- [AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias](https://arxiv.org/abs/1810.01943)
- [Mitigating Unwanted Biases with Adversarial Learning](https://arxiv.org/abs/1801.07593)


## Adversarial robustness
- [MaskedNet (protecting from Differential Power Analysis attacks](https://arxiv.org/abs/1910.13063)
- [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)
- [Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152)

## Automatic speech recognition
- [CTC](https://www.cs.toronto.edu/~graves/icml_2006.pdf)
- [RNN-T](https://arxiv.org/pdf/1211.3711.pdf)
- [Conformer](https://arxiv.org/abs/2005.08100)

## Other interesting techniques
- [Curriculum Learning](https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf)
- [Visualizing CNNs](https://arxiv.org/abs/1311.2901)
- [Grad-CAM](https://arxiv.org/abs/1610.02391)
- [Fooling CNNs](https://arxiv.org/abs/1412.1897)
- [Deep Compression](https://arxiv.org/abs/1510.00149)
- [XNOR-Net](https://arxiv.org/abs/1603.05279)
- [Data Echoing](https://arxiv.org/abs/1907.05550)
- [Bayesian Inference for Large Scale Image Classification](https://arxiv.org/abs/1908.03491)
- [Visualizing the Loss Landscape of Neural Nets (skip connections)](https://arxiv.org/abs/1712.09913)
- [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)
- [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629v2)
- [Machine Learning on Graphs: A Model and Comprehensive Taxonomy](https://arxiv.org/abs/2005.03675)
- [Understanding Neural Networks Through Deep Visualization](https://arxiv.org/abs/1506.06579)
- [Federated Learning](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)
- [Generative Models for Effective ML on Private, Decentralized Datasets](https://iclr.cc/virtual_2020/poster_SJgaRA4FPH.html)
- [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318)
- [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)
- [Graph Networks](https://arxiv.org/abs/1806.01261)
- [Relational Graph Convolutional Networks](https://arxiv.org/abs/1703.06103)
- [Learning From Brains How to Regularize Machines](https://arxiv.org/abs/1911.05072)
- [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)
- [Understanding intermediate layers using linear classifier probes](https://arxiv.org/abs/1610.01644)
- Double Descent [post](https://openai.com/blog/deep-double-descent/) and [paper](https://arxiv.org/abs/1912.02292)
- [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](https://arxiv.org/abs/2002.11794)
- [Bayesian Deep Learning and a Probabilistic Perspective of Generalization](https://arxiv.org/abs/2002.08791)
- [Learning Concepts with Energy Functions](https://openai.com/blog/learning-concepts-with-energy-functions/)
- [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://arxiv.org/abs/1908.09791)
- [DeepLIFT: Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1704.02685)
- [Auto-Encoding Variational Bayes (variational autoencoders)](https://arxiv.org/abs/1312.6114)
- [beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework](https://openreview.net/forum?id=Sy2fzU9gl)
- [The Reversible Residual Network: Backpropagation Without Storing Activations](https://arxiv.org/abs/1707.04585)
- [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)
- [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730)

## Other
- [BatchNorm Only](https://arxiv.org/abs/2003.00152)
- [A Metric Learning Reality Check](https://arxiv.org/abs/2003.08505)
- [Explainable Machine Learning in Deployment](https://arxiv.org/abs/1909.06342)
- POET [post](https://eng.uber.com/poet-open-ended-deep-learning/) and [paper](https://arxiv.org/abs/1901.01753), Enhanced POET [post](https://eng.uber.com/enhanced-poet-machine-learning/) and [paper](https://arxiv.org/abs/2003.08536)
- [Searching for Activation Functions](https://arxiv.org/abs/1710.05941)
- [Evolving Normalization-Activation Layers](https://arxiv.org/abs/2004.02967)
- [Markov Logic Networks](https://homes.cs.washington.edu/~pedrod/papers/mlj05.pdf)
- [Capsule Networks](https://arxiv.org/abs/1710.09829)
- [Stacked Capsule Autoencoders](https://arxiv.org/abs/1906.06818)
- [Collection of papers on graph neural networks](https://github.com/thunlp/GNNPapers)
- [Meta-Learning Requires Meta-Augmentation](https://arxiv.org/abs/2007.05549)
- [Geometric deep learning: going beyond Euclidean data](https://arxiv.org/abs/1611.08097)
- [A GitHub repo of deep learning papers](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap)
- [Composing graphical models with neural networks for structured representations and fast inference](https://arxiv.org/abs/1603.06277)
- [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)
- [Invertible Residual Networks](https://arxiv.org/abs/1811.00995)
- [Avoiding pathologies in very deep networks](https://arxiv.org/abs/1402.5836)
- [Pointer Networks](https://arxiv.org/abs/1506.03134)
- [Weight Agnostic Neural Networks](https://arxiv.org/abs/1906.04358)
- [Billion-scale similarity search with GPUs (FAISS)](https://arxiv.org/abs/1702.08734)
- [The Hardware Lottery](https://arxiv.org/abs/2009.06489)
- [Are All Layers Created Equal?](https://arxiv.org/abs/1902.01996)
- [Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://arxiv.org/abs/2203.05482)

## Hyperparameter Optimization
-  Population Based Training of Neural Networks [post](https://deepmind.com/blog/article/population-based-training-neural-networks) and [paper](https://arxiv.org/abs/1711.09846)
- [Practical Bayesian Optimization of MachineLearning Algorithms](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)
- [GitHub BayesianOptimization](https://github.com/fmfn/BayesianOptimization)
- [ASHA: A System for Massively Parallel Hyperparameter Tuning](https://arxiv.org/abs/1810.05934)
- hyperopt [GitHub](https://github.com/hyperopt/hyperopt) and [paper](http://proceedings.mlr.press/v28/bergstra13.pdf)
- [Ray (Tune)](https://docs.ray.io/en/latest/tune/index.html)

## Development Process
- [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://research.google/pubs/pub46555/)
- [Rules of Machine Learning: Best Practices for ML Engineering](https://developers.google.com/machine-learning/guides/rules-of-ml)

## Reinforcement Learning
- [AlphaStar](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)
- [Sutton](http://incompleteideas.net/book/RLbook2018.pdf)
- Unsupervised Meta-Learning for Reinforcement Learning [paper](https://arxiv.org/abs/1806.04640) and [blog post](https://blog.ml.cmu.edu/2020/05/01/unsupervised-meta-learning:-learning-to-learn-without-supervision/)

## Books available for free online
- [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
- [The 100 page machine learning book](https://github.com/ZakiaSalod/The-Hundred-Page-Machine-Learning-Book)
- [Python Machine Learning, Sebastian Raschka](https://github.com/rasbt/python-machine-learning-book-2nd-edition)
- [Mining of Massive Datasets](http://www.mmds.org/)
- [Deep Learning](http://www.deeplearningbook.org/)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [Think Bayes](https://greenteapress.com/wp/think-bayes/)
- [Probabilistic Programming & Bayesian Methods for Hackers](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)
- [Reinforcement Learning](http://incompleteideas.net/book/RLbook2018.pdf)
- [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

## Interesting applications
- Translating Long-Lost Languages [(article)](https://www.technologyreview.com/s/613899/machine-learning-has-been-used-to-automatically-translate-long-lost-languages/) and [paper](https://arxiv.org/abs/1906.06718)
- Identifying crop lodging [article](https://arxiv.org/abs/1906.07771)
- Pluribus (Poker) [article](https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/)
- [Evolutionary selection for self-driving cars](https://deepmind.com/blog/article/how-evolutionary-selection-can-train-more-capable-self-driving-cars)
- [MuZero](https://arxiv.org/abs/1911.08265)

## General AI topics
- [State of AI (Nathan Benaich)](https://www.stateof.ai/)
- [Statistical Modeling: The Two Cultures](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726)
- [On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)
- [Software 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35)
- [Shortcut Learning](https://arxiv.org/abs/2004.07780)
- [Tips for Research](https://ruder.io/10-tips-for-research-and-a-phd/)
- [An Opinionated Guide to ML Research](http://joschu.net/blog/opinionated-guide-ml-research.html)
- [De-Mystifying Good Research and Good Papers](https://bigaidream.gitbooks.io/tech-blog/content/2014/de-mystifying-good-research.html)
- [Why We Need DevOps for ML Data](https://tecton.ai/blog/devops-ml-data/)
- [AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence](https://arxiv.org/abs/1905.10985)
